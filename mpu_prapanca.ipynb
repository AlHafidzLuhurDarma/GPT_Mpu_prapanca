{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementasi pada Mpu-Prapanca"
      ],
      "metadata": {
        "id": "GkkO0acicSfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okNlrLgp5xvF",
        "outputId": "486b5030-e946-49d1-c5eb-b326f460a6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.4443, val loss 4.4305\n",
            "Step 300: train loss 1.6273, val loss 1.9686\n",
            "Step 600: train loss 1.1797, val loss 1.6356\n",
            "Step 900: train loss 0.9775, val loss 1.6093\n",
            "Step 1200: train loss 0.8042, val loss 1.6270\n",
            "Step 1500: train loss 0.6769, val loss 1.7539\n",
            "Step 1800: train loss 0.5534, val loss 1.9015\n",
            "Step 2100: train loss 0.4542, val loss 2.0352\n",
            "Step 2400: train loss 0.3978, val loss 2.2546\n",
            "Step 2700: train loss 0.3455, val loss 2.3562\n",
            "\n",
            "Pada waktu sunyi orang telah tidur, dikejar dan diamuk lagi oleh Nusapati.\n",
            "Kata orang orang Batil: \"Sudah wafatlah tuan waktu ia menuju ke pergi, jika sudah kereta, gugur, darah seperti rubuh oleh gunung Tugaran Singasari, sempurna takut berkata: \"Tuanku, semengamur di jalan, demikian persamaanya.\n",
            "Lalu itu pagi kekatang saja. Keris berada dian pengelah Gajahpara, hari mengira, sendiring mati oleh hamba hamba Madunan, anaknya seorang kelanjutnya Dewa.\"\n",
            "Utusanya demikian persembuatannya saja.\"\n",
            "Pad\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "'''\n",
        "# hyperparameter\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd= 384\n",
        "\n",
        "Step 2700: train loss 1.6100, val loss 1.9787  --> block_size = 16\n",
        "Step 2700: train loss 1.1997, val loss 1.6784  --> block_size = 16, n_embd=128\n",
        "Step 2700: train loss 0.9533, val loss 1.5886  --> block_size = 32, n_embd=128\n",
        "Step 2700: train loss 0.7735, val loss 1.7474  --> block_size = 32, n_embd=256\n",
        "Step 2700: train loss 0.3507, val loss 2.3076  --> block_size = 64, n_embd=256\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "n_embd=32\n",
        "'''\n",
        "# hyperparameter\n",
        "batch_size = 32\n",
        "block_size = 64\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd= 256\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "path = \"/content/drive/MyDrive/Pararaton_Mpu_Prapanca.txt\"\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "# tokenization\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters that occur in this text\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "# train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data input x and target y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ('train', 'val'):\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    # we want to intersperses communitation and then computation\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)  # communication\n",
        "    self.ffwd = FeedForward(n_embd)                  # computation\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        nn.LayerNorm(n_embd),\n",
        "    )\n",
        "    #self.sa_head = MultiHeadAttention(4, n_embd//4)  # i.e. 4 heads of 8-dimensional self-attention\n",
        "    #self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    # idx and targets are both (B, T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)  --> C in here is n_emb\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "    x = tok_emb + pos_emb   #(B, T, C)\n",
        "    x = self.blocks(x) # (B, T, C)\n",
        "    #x = self.sa_head(x)\n",
        "    #x = self.ffwd(x)  # (B, T, C)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      # foucus only on the last time step\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index into running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.randint(3, (2, 2), device=device)\n",
        "print(context)\n",
        "print(decode(m.generate(context, max_new_tokens=50)[0].tolist()))"
      ],
      "metadata": {
        "id": "cGc4IIMx6da8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a6df23-be9a-495a-99b0-a6272ae132ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1],\n",
            "        [0, 1]], device='cuda:0')\n",
            "\n",
            " ada waktu aku memuja\".\n",
            "Menjawablah Ken Angrok: \"Be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.randint(50, (5,), device=device)\n",
        "print(context)\n",
        "print(context[0])\n",
        "print(context[0].tolist())\n",
        "print(decode(context.tolist()))"
      ],
      "metadata": {
        "id": "p_ZsntC-9ORs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb14c60-e199-4fef-d090-d94462911c15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([39,  6,  8,  9, 34], device='cuda:0')\n",
            "tensor(39, device='cuda:0')\n",
            "39\n",
            "U-01O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor((1, 1), dtype=torch.long, device=device)"
      ],
      "metadata": {
        "id": "ont1v1t5BwfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9196433b-b1ea-47e6-fe4e-77dfca3689c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1]])"
      ],
      "metadata": {
        "id": "JSTJQw5rENx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d94a678-e8fa-4c08-de9c-e29b52f30ff0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = 'raja'\n",
        "print(encode(my_text))\n",
        "text = torch.tensor([encode(my_text)], dtype=torch.long, device=device)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "Ht9dNdLR93QM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5918af-4b45-476e-e327-ad22acd31507"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60, 44, 53, 44]\n",
            "tensor([[60, 44, 53, 44]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(text, max_new_tokens=50)[0].tolist()))"
      ],
      "metadata": {
        "id": "U3CjDLiNAAWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6d2942-d3e0-452b-fb94-2fbe8a72586d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raja merusuh.\n",
            "Adalah seorang neDewa, Bangka, Mpu Palot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(text, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlfM_jRFfV9V",
        "outputId": "72a82271-d2d3-438c-c333-5a65000a9f24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raja tidur. Raden Wijaya ditusukkan, hendaknyalah kamu wahai utusan, buyung Angrok, akhirnya mati oleh gemba.\" Segera Singasarilah waktu lahir ada dewa: \"Yang bagan buah jambu tanda baik, sayembah kepada keris kepada daerah lingkungan kutu: Ken Angrok, semua tingkah laku Ken Endok, sedang cinta mencintai para kepada hari raya, hari tiga, hari dua, hari sif, pergilah yang mengejar. Ketersesampatan waktu ia menuju ke Jawa, tidak berlah yang menggantinya, lalu diberi raja Mantrolot itu, memanggil buru \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(text, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM9H37SpfYiG",
        "outputId": "3d95a4ae-8090-4065-d545-13e3df370b16"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raja, tak laki Adapun di Tuyantapang daerah lingkungan Bapa, kemudian dijumpai anaknya hamba kerjanjian itu, kalau tuan pulang, kalau Ken Angrok memang sungguh sungga rah: \"masih pada waktu seorang kelamaan ketahuannya, bergantung di Tumapel.\n",
            "Sang Anusapati mempunyai seorang anak, : \"Baiklah, kakan kuda masuk kedalam istana Daha, untuk melarikan puteri bangsawan, hendaknyalah kamu menyuruh membuat keris kepadanya, sungguh ia bersediga.\n",
            "Sesudah Sang Amurwabumi sebuara perempuan yang diindahkan melihat penuh, gugur, tak melamakan pembalasan, tidak, saya akan tahu kanan kamu, buyung, agar supaya hamba dapat menghamba kepada Tunggul Ametung oleh Ken Endok, maka keluar aingku dan didalam soal soal yang luar biasa.\"\n",
            "Ken Angrok pergi dari Karuman.\n",
            "Kata kepada Ken Angrok: \"Betulah tuanku kepada karena jumlah orang yang tidur, ditusuk oleh Tuan, tak untuk didjadikan dan persekutuannya: \"Nah, bawa berjudi di Tumapel, nama nobatan Batara Guru, demikian itu pujian dari dewaktu ia orang yang menggembal\n"
          ]
        }
      ]
    }
  ]
}